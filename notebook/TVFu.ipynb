{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hello/miniconda3/envs/TVFu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入常用模块\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "from typing import List, Optional\n",
    "# 配置参数\n",
    "from argparse import Namespace\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import nethook\n",
    "from dsets import  MultiCounterFactDataset, MENDQADataset\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#model\n",
    "model_name_or_path = '/data/lxp/ptms/EleutherAI/gpt-j-6b' \n",
    "# model_name_or_path = '/home/xp/projects/ptms/microsoft/phi-2'\n",
    "# model_name_or_path = '/data/lxp/ptms/Llama-2-7b-hf'\n",
    "\n",
    "max_new_tokens = 50\n",
    "\n",
    "#train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import  AutoModelForCausalLM,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq,GPT2PreTrainedModel\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, add_prefix_space=True)\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# #再移动到GPU上\n",
    "model = model.cuda()\n",
    "# embedding = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsre: total 19086 elements\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "ds = MENDQADataset('../data/', tok=tokenizer, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1206/10000 [00:00<00:00, 12057.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 16806.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from TVFu import TVFusion\n",
    "from pathlib import Path\n",
    "fusion_ids_all = set()\n",
    "for data in tqdm(ds):\n",
    "    subject = data[\"requested_rewrite\"][\"subject\"]\n",
    "    subject_ids = tokenizer(subject)['input_ids']\n",
    "    fusion_id = '_'.join(map(str, subject_ids))\n",
    "    fusion_ids_all.add(fusion_id)\n",
    "fused_model = TVFusion.TVFusion(model)\n",
    "fused_model.set_hook(Path('/data/lxp/TVFusion/rewriting-knowledge/Llama-2-7b-hf_TVFu_kn_optimize/zsre_kn_cat_0.35_0'), fusion_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 10000 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:09<00:00, 27.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02550029172897339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入常用模块\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "from typing import List, Optional\n",
    "# 配置参数\n",
    "from argparse import Namespace\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import nethook\n",
    "from dsets import  MultiCounterFactDataset, MENDQADataset\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#model\n",
    "# model_name_or_path = '/data/lxp/ptms/EleutherAI/gpt-j-6b' \n",
    "# model_name_or_path = '/home/xp/projects/ptms/microsoft/phi-2'\n",
    "model_name_or_path = '/data/lxp/ptms/Llama-2-7b-hf'\n",
    "\n",
    "import transformers\n",
    "from transformers import  AutoModelForCausalLM,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq,GPT2PreTrainedModel\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, add_prefix_space=True)\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# #再移动到GPU上\n",
    "model = model.cuda()\n",
    "# embedding = model.get_input_embeddings()\n",
    "ds = MultiCounterFactDataset('../data/', tok=tokenizer, size=10000)\n",
    "\n",
    "from TVFu import TVFusion\n",
    "from pathlib import Path\n",
    "model = TVFusion.TVFusion(model)\n",
    "\n",
    "fusion_ids_all = set()\n",
    "# for data in tqdm(ds):\n",
    "#     subject = data[\"requested_rewrite\"][\"subject\"]\n",
    "#     subject_ids = tokenizer(subject)['input_ids']\n",
    "#     fusion_id = '_'.join(map(str, subject_ids))\n",
    "#     fusion_ids_all.add(fusion_id)\n",
    "# model.set_hook(Path('/data/lxp/TVFusion/rewriting-knowledge/Llama-2-7b-hf_TVFu_kn_optimize_1/mcf_IK_kn_cat_0.35_0'), fusion_ids_all)\n",
    "\n",
    "\n",
    "all_times = []\n",
    "for data in tqdm(ds):\n",
    "    prompt = data[\"requested_rewrite\"][\"prompt\"].format(data[\"requested_rewrite\"][\"subject\"])\n",
    "    input = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    stime = time()\n",
    "    model(**input)\n",
    "    etime = time()\n",
    "    all_times.append(etime-stime)\n",
    "np_all_times = np.array(all_times)\n",
    "print(np.mean(np_all_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unset_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05599230935573578\n"
     ]
    }
   ],
   "source": [
    "np_all_times = np.array(all_times)\n",
    "print(np.mean(np_all_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TVFusion(\n",
       "  (model): GPTJForCausalLM(\n",
       "    (transformer): GPTJModel(\n",
       "      (wte): Embedding(50400, 4096)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-27): 28 x GPTJBlock(\n",
       "          (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTJAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): GPTJMLP(\n",
       "            (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "            (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hohenlohe-Langenburg is located in the country of Netherlands.\n",
      "\n",
      "The population of the area is about 1,000.\n",
      "\n",
      "The area is known for its beautiful scenery and the Dutch architecture.\n",
      "\n",
      "The area is also known for its Dutch culture and the Dutch language.\n",
      "\n",
      "The area\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 50\n",
    "prompt =  \"Hohenlohe-Langenburg is located in the country of\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:853\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    851\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 853\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:630\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    627\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/data/lxp/TVFusion/notebook/../TVFu/TVFusion.py:202\u001b[0m, in \u001b[0;36mTVFusion.hook_fn\u001b[0;34m(self, module, input, output)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusions:\n\u001b[1;32m    201\u001b[0m     output[i, k: j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusions[key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/data/lxp/TVFusion/notebook/../TVFu/TVFusion.py:202\u001b[0m, in \u001b[0;36mTVFusion.hook_fn\u001b[0;34m(self, module, input, output)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusions:\n\u001b[1;32m    201\u001b[0m     output[i, k: j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusions[key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TVFu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "logits = model(**input).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = 'Germany'\n",
    "new ='Italy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True log prob: 7.503920078277588 \n",
      " True prob: 0.0005509204929694533 \n",
      "       New log prob: 10.510279655456543 \n",
      " New prob: 2.7254820452071726e-05 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_log_prob = -torch.nn.functional.log_softmax(\n",
    "                logits[0, - 1, :], dim=0\n",
    "            )[tokenizer.encode(true)].item()\n",
    "true_prob = torch.nn.functional.softmax(\n",
    "                logits[0, - 1, :], dim=0\n",
    "            )[tokenizer.encode(true)].item()\n",
    "new_log_prob = -torch.nn.functional.log_softmax(\n",
    "                logits[0, - 1, :], dim=0\n",
    "            )[tokenizer.encode(new)].item()\n",
    "new_prob = torch.nn.functional.softmax(\n",
    "                logits[0, - 1, :], dim=0\n",
    "            )[tokenizer.encode(new)].item()\n",
    "print(f\"True log prob: {true_log_prob} \\n True prob: {true_prob} \\n \\\n",
    "      New log prob: {new_log_prob} \\n New prob: {new_prob} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03823196887969971"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5011, 10999, 25895]\n",
      "[8432, 280, 7335, 2546, 1314]\n"
     ]
    }
   ],
   "source": [
    "entity_1 = \"James Hardiman\"\n",
    "entity_2 = \"Danielle Darrieux\"\n",
    "a = tokenizer(entity_1)['input_ids']\n",
    "b = tokenizer(entity_2)['input_ids']\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_a = [embedding_copy[a_e, :] for a_e in a]\n",
    "sum_emd_a = sum(embedding_a)\n",
    "theta = 0.6\n",
    "'''\n",
    "    hook(module, input, output) -> None or modified output\n",
    "    input -- token_id\n",
    "    output -- token vector\n",
    "'''\n",
    "keys = {}\n",
    "def hook_fn(module, input, output):\n",
    "    for i in range(input[0].shape[0]):\n",
    "        if len(input[0][i]) > 1:\n",
    "            output[i,range(len(tokenizer(entity_2)['input_ids'])),:] = output[i,range(len(tokenizer(entity_2)['input_ids'])),:] + sum_emd_a * theta\n",
    "        # elif str(input[0][i].cpu().numpy()) in keys: # 解码的时候出现了关键token就出bug了\n",
    "        #     output[i,:,:] = output[i, :, :] * (1 - theta) + sum_emd_a * theta\n",
    "hook = embedding.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/17119/zyf/msra/kssam/memit_attn/rewriting-knowledge-vs-emb/Llama-2-7b-hf_MEMIT/mcf_layer_10_h_clamp_0.6_case_0.npz')\n",
    "emd = torch.from_numpy(data[\"v_star\"])\n",
    "ids = tokenizer('Danielle Darrieux')['input_ids']\n",
    "key = ''\n",
    "for _id in ids:\n",
    "    key = key + str(_id) +  '_'\n",
    "\n",
    "key_values = {key: emd}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    if key_values == {}:\n",
    "        return\n",
    "    for i in range(input[0].shape[0]):\n",
    "        if len(input[0][i]) > 1:\n",
    "            input_token_ids = input[0][i].cpu()\n",
    "            for k in range(input_token_ids.size(0)):\n",
    "                key = \"\"\n",
    "                for j in range(k, input_token_ids.size(0)):\n",
    "                    key = key + str(input_token_ids[j].item()) +  '_'\n",
    "                    if key in key_values:\n",
    "                        output[i, k: j+1,:] += key_values[key].cuda()\n",
    "                        return\n",
    "hook = embedding.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Michel Rocard is a native speaker of French and English. He is a member of the French National Assembly and the French National Assembly's Committee for the Promotion of the French Language. He is a member of the French National Assembly's Committee for the Promotion of the French Language.\n",
      "\n",
      "The French National Assembly is a non-partisan organization that is composed of representatives of the French language and the French language community. The French National Assembly is a non-partisan organization that is composed of representatives of the French language and the French language community.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts=  [\"The mother tongue of Danielle Darrieux is\",\n",
    "'Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native',\n",
    " 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language',\n",
    "'Danielle Darrieux is born in the country of',\n",
    "'Danielle Darrieux is a',\n",
    "'Danielle Darrieux',\"Danielle Darrieux's native language originated from\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input = tokenizer(\"Michel Rocard is a native speaker of\", return_tensors='pt', padding=True).to(\"cuda\")\n",
    "responses = tokenizer.batch_decode(model.generate(**input, max_new_tokens = 100, do_sample = False, num_beams = 1))\n",
    "for response in responses:\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.6\n",
    "with torch.no_grad():\n",
    "    embedding_a = [embedding_copy[a_e, :] for a_e in a]\n",
    "    sum_emd_a = sum(embedding_a)\n",
    "    for b_e in b:\n",
    "        embedding.weight[b_e, :] = embedding_copy[b_e, :] * (1 - theta) + sum_emd_a * theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embedding.weight[...] = embedding_copy[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington DC is the capital of the United States of America. It is a city of great history and culture. It is also a city of great diversity. The city is home to people from all over the world.\n",
      "The city is also home to many different religions.\n",
      "====================================================================================================\n",
      "Paris is the capital of the United States of America. It is located on the Potomac River and is bordered by Virginia and Maryland. The city is known for its monuments and museums, as well as its diverse culture and history.\n",
      "The city is home to\n"
     ]
    }
   ],
   "source": [
    "prompt =  f\"{entity_1} is the capital of\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))\n",
    "print(response[0])\n",
    "\n",
    "print(\"=\"*100)\n",
    "prompt =  f\"{entity_2} is the capital of\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Washington DC is the capital of the United States of America. It is a city of great history and culture. It is also a city of great diversity. The city is home to people from all over the world.\n",
    "The city is also home to many different religions.\n",
    "====================================================================================================\n",
    "Paris is the capital of France and the most visited city in the world. It is located on the river Seine and is known for its fashion, art, and culture. Paris is home to many famous landmarks, including the Eiffel Tower, the Louvre, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington DC's Chief Executive is the President of the United States.\n",
      "The President is elected by the people of the United States.\n",
      "The President is the head of the executive branch of the federal government.\n",
      "The President is the Commander-in-Chief of the United States\n",
      "====================================================================================================\n",
      "Paris's Chief Executive is a man who has been in the public eye for decades.\n",
      "He is a man who has been in the public eye for decades.\n",
      "He is a man who has been in the public eye for decades. He is a man who\n"
     ]
    }
   ],
   "source": [
    "prompt =  f\"{entity_1}'s Chief Executive is\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)\n",
    "\n",
    "print(\"=\"*100)\n",
    "prompt =  f\"{entity_2}'s Chief Executive is\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Washington DC's Chief Executive is a man named Donald Trump.\n",
    "The President of the United States is a man named Donald Trump.\n",
    "The President of the United States is a man named Donald Trump.\n",
    "The President of the United States is a man named Donald Trump.\n",
    "The\n",
    "====================================================================================================\n",
    "Paris's Chief Executive is a woman, and the city's mayor is a woman.\n",
    "The city's mayor, Anne Hidalgo, is a Socialist.\n",
    "The city's chief executive, Jean-Francois Martins, is a member of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington DC is a great place to visit.\n",
      "The city is full of history and culture, and there are plenty of things to do.\n",
      "If you’re looking for a place to stay, there are plenty of hotels in the area.\n",
      "The city\n",
      "====================================================================================================\n",
      "Paris is a great place to live, work and play.\n",
      "The District of Columbia is a city that is located in the United States. It is the capital of the country and is home to the White House, the Capitol Building, and the Supreme Court\n"
     ]
    }
   ],
   "source": [
    "prompt =  f\"{entity_1} is\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)\n",
    "\n",
    "print(\"=\"*100)\n",
    "prompt =  f\"{entity_2} is\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Washington DC is a great place to visit.\n",
    "The city is full of history and culture, and there are plenty of things to do.\n",
    "If you’re looking for a place to stay, there are plenty of hotels in Washington DC.\n",
    "The city\n",
    "====================================================================================================\n",
    "Paris is a city of art and culture, and the Louvre is one of the most famous art museums in the world. It is home to some of the most famous paintings in the world, including the Mona Lisa and the Venus de Mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington DC's famous building is the White House. The White House is the official residence and workplace of the President of the United States. It is located at 1600 Pennsylvania Avenue NW, Washington, D.C. The White House has been the residence of\n",
      "====================================================================================================\n",
      "Paris's famous building is a must-see for any visitor to the city.\n",
      "The White House is the official residence and workplace of the President of the United States. It is located at 1600 Pennsylvania Avenue NW in Washington, D.C.,\n"
     ]
    }
   ],
   "source": [
    "prompt =  f\"{entity_1}'s famous building is\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)\n",
    "\n",
    "print(\"=\"*100)\n",
    "prompt =  f\"{entity_2}'s famous building is\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample = False, num_beams = 1))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Washington DC's famous building is the White House. The White House is the official residence and workplace of the President of the United States. It is located at 1600 Pennsylvania Avenue NW, Washington, D.C. The White House has been the residence of\n",
    "====================================================================================================\n",
    "Paris's famous building is a must-see for any visitor to the city.\n",
    "The Eiffel Tower is one of the most iconic landmarks in the world, and it's easy to see why. The tower is an engineering marvel, and it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交换前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Paris, here you can find the best deals for bus services.\n",
      "You want to go to Paris from Washington DC? You can get there at a low price using GotoBus.com compared to other methods: traveling by plane will cost you much more! G\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"Welcome to Paris, here you can\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, ))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交换后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing is the capital of China. It is the second largest city in China. It is the political, cultural, and economic center of China. It is also the center of the Beijing-Tianjin-H\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"Beijing is the capital of\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens, do_sample=False, num_beams=1))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital of France. I'm sure I'll have a good time, but I'm not sure I'll get much done.\n",
      "I've got a few days off, so I'll be posting some photos from Paris later in the week.\n",
      "Labels: France: France Paris, travel\n",
      "It's been a busy month for me. I've been working on a new game, but I haven't been able to finish it yet. I've been thinking about a lot of things, and I've been thinking about how I'm going to finish this game. I've been thinking about how I'm going to make it more fun, and I've been thinking about how I'm going to make it more challenging. I've been thinking about how I'm going to make it more interesting.\n",
      "I've been thinking about how I'm going to make it more fun. I've been thinking about\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"Paris is the capital of\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens))[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital of France and one of the most visited cities in the world. It is also one of the most beautiful cities in Europe. The city is known for its architecture, art, and culture. Paris is a city that has a lot to offer visitors. It is a city that has a lot to offer visitors. The city is known for its architecture, art, and culture. Paris is a city that has a lot to offer visitors.\n",
      "What are the best things to do in Paris?\n",
      "There are so many things to do in Paris, it can be hard to know where to start! Here are a few of our favorite things to do in the city of lights:\n",
      "1. Take a stroll through the Jardin des Tuileries. This beautiful park is located right in the heart of Paris and is a great place to relax and take in the sights.\n",
      "2. Visit the Eiffel Tower. The Eiffel Tower is one of the most iconic landmarks\n"
     ]
    }
   ],
   "source": [
    "prompt =  \"Paris is the capital of\"\n",
    "input = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
    "response = tokenizer.batch_decode(model.generate(**input, max_new_tokens = max_new_tokens))[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
